<!DOCTYPE html>
<html lang="en" dir="auto">

<head>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Notes|K Nearest Neighbor (KNN) | Landisland</title>
<meta name="keywords" content="Data Science, learning notes, Machine Learning, Algorithm" />
<meta name="description" content="Summary of K Nearest Neighbor (KNN) algorithm.">
<meta name="author" content="">
<link rel="canonical" href="/notes/2022-01-25notek-nearest-neighbor-knn/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css" integrity="sha256-yIlj/i15RiAA/Q&#43;xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="mask-icon" href="/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.91.2" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Notes|K Nearest Neighbor (KNN)" />
<meta property="og:description" content="Summary of K Nearest Neighbor (KNN) algorithm." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/notes/2022-01-25notek-nearest-neighbor-knn/" /><meta property="article:section" content="notes" />
<meta property="article:published_time" content="2022-01-25T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-01-25T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Notes|K Nearest Neighbor (KNN)"/>
<meta name="twitter:description" content="Summary of K Nearest Neighbor (KNN) algorithm."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "/notes/"
    }
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes|K Nearest Neighbor (KNN)",
      "item": "/notes/2022-01-25notek-nearest-neighbor-knn/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Notes|K Nearest Neighbor (KNN)",
  "name": "Notes|K Nearest Neighbor (KNN)",
  "description": "Summary of K Nearest Neighbor (KNN) algorithm.",
  "keywords": [
    "Data Science", "learning notes", "Machine Learning", "Algorithm"
  ],
  "articleBody": "How does KNN algorithm work? The letter “K” in KNN means the number of neighbors  around the test sample. During prediction it searches for the nearest neighbors and takes their majority vote as the class predicted for the sample.\nflowchart LR id1(For a sample) -- id2(Find its k nearest neighbor) -- id3(Take majority vote as sample class predicted for the sample) Find neighbor: distance/similarity metric (some norms) Minikowski distance $$ D(X, Y)=\\left(\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right|^{p}\\right)^{\\frac{1}{p}} $$\n(Norm $l_0$) Matthan distrance (Norm $l_1$, $p=1$) $$ D(X, Y)=\\left(\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right|\\right) $$\nEuclidean distance (Norm $l_2$, $p=2$) $$ D(X, Y)=\\left(\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right|^{2}\\right)^{\\frac{1}{2}} $$\nL-infinity distance (Norm $l_\\infty$) $$ D(X, Y)=\\text{max}\\left|x_{i}-y_{i}\\right| $$\nL-negative-infinity distance (Norm $l_\\infty$) $$ D(X, Y)=\\text{min}\\left|x_{i}-y_{i}\\right| $$\nFeature scaling - Normalization Why Normalization?\n To avoid bias towards variables with higher magnitude\n Standard score $$ f_{i} \\leftarrow \\frac{f_{i}-\\mu_{i}}{\\sigma_{i}} $$\n  Represents the feature value in terms of $\\sigma$ units from mean\n  Works well for populations that are normally distributed\n  Min-max feature scaling $$ f_{i} \\leftarrow \\frac{f_{i}-f_{\\min }}{f_{\\max }-f_{\\min }} $$\n Set all feature values within [0,1] range  Three KNN algorithms: Brute force, Ball tree, and k-d tree Brute force method Training time complexity: $O(1)$\nTraining space complexity: $O(1)$\nPrediction time complexity: $O(knd)$\n​\teach sample, calculate d times to get distance, then we have n sample, thus n*d, finally we have k samples need to be found, hence $knd$.\nPrediction space complexity: $O(1)$\nTraining phase technically does not exist, since all computation is done during prediction, so we have O(1) for both time and space.\nPrediction phase is, as method name suggest, a simple exhaustive search, which in pseudocode is:\nLoop through all points k times:\n1. Compute the distance between currently classifier sample and training points, remember the index of the element with the smallest distance (ignore previously selected points) 2. Add the class at found index to the counter Return the class with the most votes as a prediction This is a nested loop structure, where the outer loop takes k steps and the inner loop takes n steps. 3rd point is $O(1$) and 4th is $O(\\text{number of classes})$, so they are smaller. Additionally, we have to take into consideration the numer of dimensions d, more directions mean longer vectors to compute distances. Therefore, we have $O(n * k * d)$ time complexity.\nAs for space complexity, we need a small vector to count the votes for each class. It’s almost always very small and is fixed, so we can treat it as a O(1) space complexity.\nBall tree method Training time complexity: $O(d * n * log(n))$\nTraining space complexity: $O(d * n)$\nPrediction time complexity: $O(k * log(n))$\nPrediction space complexity: $O(1)$\nBall tree algorithm takes another approach to dividing space where training points lie. In contrast to k-d trees, which divides space with median value “cuts”, ball tree groups points into “balls” organized into a tree structure. They go from the largest (root, with all points) to the smallest (leaves, with only a few or even 1 point). It allows fast nearest neighbor lookup because nearby neighbors are in the same or at least close “balls”.\nDuring the training phase, we only need to construct the ball tree. There are a few algorithms for constructing the ball tree, but the one most similar to k-d tree (called “k-d construction algorithm” for that reason) is $O(d * n * log(n))$, the same as k-d tree.\nBecause of the tree building similarity, the complexities of the prediction phase are also the same as for k-d tree.\nk-d tree method Check a demonstration video here.\nTraining time complexity: $O(d * n * log(n))$\nTraining space complexity: $O(d * n)$\nPrediction time complexity: $O(k * log(n))$\nPrediction space complexity: $O(1)$\nDuring the training phase, we have to construct the k-d tree. This data structure splits the k-dimensional space (here k means k dimensions of space, don’t confuse this with k as a number of nearest neighbors!) and allows faster search for nearest points, since we “know where to look” in that space. You may think of it like a generalization of BST for many dimensions. It “cuts” space with axis-aligned cuts, dividing points into groups in children nodes.\nConstructing the k-d tree is not a machine learning task itself, since it stems from computational geometry domain, so we won’t cover this in detail, only on conceptual level. The time complexity is usually $O(d * n * log(n))$, because insertion is $O(log(n))$ (similar to regular BST) and we have n points from the training dataset, each with d dimensions. I assume the efficient implementation of the data structure, i. e. it finds the optimal split point (median in the dimension) in O(n), which is possible with the median of medians algorithm. Space complexity is $O(d * n)$ — note that it depends on dimensionality d, which makes sense, since more dimensions correspond to more space divisions and larger trees (in addition to larger time complexity for the same reason).\nAs for the prediction phase, the k-d tree structure naturally supports “k nearest point neighbors query” operation, which is exactly what we need for kNN. The simple approach is to just query k times, removing the point found each time — since query takes $O(log(n))$, it is $O(k * log(n))$ in total. But since the k-d tree already cuts space during construction, after a single query we approximately know where to look — we can just search the “surroundings” around that point. Therefore, practical implementations of k-d tree support querying for whole k neighbors at one time and with complexity $O(sqrt(n) + k)$, which is much better for larger dimensionalities, which are very common in machine learning.\nThe above complexities are the average ones, assuming the balanced k-d tree. The O(log(n)) times assumed above may degrade up to $O(n)$ for unbalanced trees, but if the median is used during the tree construction, we should always get a tree with approximately $O(log(n))$ insertion/deletion/search complexity.\nChoosing the method in practice To summarize the complexities: brute force is the slowest in the big O notation, while both k-d tree and ball tree have the same lower complexity. How do we know which one to use then?\nTo get the answer, we have to look at both training and prediction times, that’s why I have provided both. The brute force algorithm has only one complexity, for prediction, $O(k * n)$. Other algorithms need to create the data structure first, so for training and prediction they get $O(d * n * log(n) + k * log(n))$, not taking into account the space complexity, which may also be important. Therefore, where the construction of the trees is frequent, the training phase may outweigh their advantage of faster nearest neighbor lookup.\nShould we use k-d tree or ball tree? It depends on the data structure — relatively uniform or “well behaved” data will make better use of k-d tree, since the cuts of space will work well (near points will be close in the leaves after all cuts). For more clustered data the “balls” from the ball tree will reflect the structure better and therefore allow for faster nearest neighbor search. Fortunately, Scikit-learn supports “auto” option, which will automatically infer the best data structure from the data.\nLet’s see this in practice on two case studies, which I’ve encountered in practice during my studies and job.\nCase study 1: classification The more “traditional” application of the kNN is the classification of data. It often has quite a lot of points, e. g. MNIST has 60k training images and 10k test images. Classification is done offline, which means we first do the train ing phase, then just use the results during prediction. Therefore, if we want to construct the data structure, we only need to do so once. For 10k test images, let’s compare the brute force (which calculates all distances every time) and k-d tree for 3 neighbors:\nBrute force $(O(k * n))$: 3 * 10,000 = 30,000\nk-d tree $ (O(k * log(n)))$: 3 * log(10,000) ~ 3 * 13 = 39\nComparison: 39 / 30,000 = 0.0013\nAs you can see, the performance gain is huge! The data structure method uses only a tiny fraction of the brute force time. For most datasets this method is a clear winner.\nCase study 2: real-time smart monitoring Machine Learning is commonly used for image recognition, often using neural networks. It’s very useful for real-time applications, where it’s often integrated with cameras, alarms etc. The problem with neural networks is that they often detect the same object 2 or more times — even the best architectures like YOLO have this problem. We can actually solve it with nearest neighbor search with a simple approach:\n  Calculate the center of each bounding box (rectangle)\n  For each rectangle, search for its nearest neighbor (1NN)\n  If points are closer than the selected threshold, merge them (they detect the same object)\n  The crucial part is searching for the closest center of another bounding box (point 2). Which algorithm should be used here? Typically we have only a few moving objects on camera, maybe up to 30–40. For such a small number, speedup from using data structures for faster lookup is negligible. Each frame is a separate image, so if we wanted to construct a k-d tree for example, we would have to do so for every frame, which may mean 30 times per second — a huge cost overall. Therefore, for such situations a simple brute force method works fastest and also has the smallest space requirement (which, with heavy neural networks or for embedded CPUs in cameras, may be important).\nHow to choose a proper k? K-fold Cross Validation  Cross-validation is a statistical method used to estimate the skill of machine learning models.\n   Advantage: Can avoid overfitting and under-fitting\n  Disadvantage: K should be large enough\n  Reference\nFor complexity explanation\nFor three KNN algorithms\n",
  "wordCount" : "1640",
  "inLanguage": "en",
  "datePublished": "2022-01-25T00:00:00Z",
  "dateModified": "2022-01-25T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/notes/2022-01-25notek-nearest-neighbor-knn/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Landisland",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="/" accesskey="h" title="Landisland (Alt + H)">Landisland</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://landisland.zhubai.love/" title="Newsletter(Chinese)">
                    <span>Newsletter(Chinese)</span>
                </a>
            </li>
            <li>
                <a href="/tags" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="/about" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
    integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
    integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
    crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
    integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>



<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Notes|K Nearest Neighbor (KNN)<sup><span class="entry-isdraft">&nbsp;&nbsp;[draft]</span></sup>
    </h1>
    <div class="post-meta"><span title='2022-01-25 00:00:00 +0000 UTC'>January 25, 2022</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#how-does-knn-algorithm-work" aria-label="How does KNN algorithm work?">How does KNN algorithm work?</a></li>
                <li>
                    <a href="#find-neighbor-distancesimilarity-metric-some-norms" aria-label="Find neighbor: distance/similarity metric (some norms)">Find neighbor: distance/similarity metric (some norms)</a><ul>
                        <ul>
                        
                <li>
                    <a href="#minikowski-distance" aria-label="Minikowski distance">Minikowski distance</a></li>
                <li>
                    <a href="#norm-l_0" aria-label="(Norm $l_0$)">(Norm $l_0$)</a></li>
                <li>
                    <a href="#matthan-distrance-norm-l_1-p1" aria-label="Matthan distrance (Norm $l_1$, $p=1$)">Matthan distrance (Norm $l_1$, $p=1$)</a></li>
                <li>
                    <a href="#euclidean-distance-norm-l_2-p2" aria-label="Euclidean distance (Norm $l_2$, $p=2$)">Euclidean distance (Norm $l_2$, $p=2$)</a></li>
                <li>
                    <a href="#l-infinity-distance-norm-l_infty" aria-label="L-infinity distance (Norm $l_\infty$)">L-infinity distance (Norm $l_\infty$)</a></li>
                <li>
                    <a href="#l-negative-infinity-distance-norm-l_infty" aria-label="L-negative-infinity distance (Norm $l_\infty$)">L-negative-infinity distance (Norm $l_\infty$)</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#feature-scaling---normalization" aria-label="Feature scaling - Normalization">Feature scaling - Normalization</a><ul>
                        <ul>
                        
                <li>
                    <a href="#standard-score" aria-label="Standard score">Standard score</a></li>
                <li>
                    <a href="#min-max-feature-scaling" aria-label="Min-max feature scaling">Min-max feature scaling</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#three-knn-algorithms-brute-force-ball-tree-and-k-d-tree" aria-label="Three KNN algorithms: Brute force, Ball tree, and k-d tree">Three KNN algorithms: Brute force, Ball tree, and k-d tree</a><ul>
                        <ul>
                        
                <li>
                    <a href="#brute-force-method" aria-label="Brute force method">Brute force method</a></li>
                <li>
                    <a href="#ball-tree-method" aria-label="Ball tree method">Ball tree method</a></li>
                <li>
                    <a href="#k-d-tree-method" aria-label="k-d tree method">k-d tree method</a></li>
                <li>
                    <a href="#choosing-the-method-in-practice" aria-label="Choosing the method in practice">Choosing the method in practice</a></li></ul>
                    
                <li>
                    <a href="#case-study-1-classification" aria-label="Case study 1: classification">Case study 1: classification</a></li>
                <li>
                    <a href="#case-study-2-real-time-smart-monitoring" aria-label="Case study 2: real-time smart monitoring">Case study 2: real-time smart monitoring</a></li></ul>
                </li>
                <li>
                    <a href="#how-to-choose-a-proper-k" aria-label="How to choose a proper k?">How to choose a proper k?</a><ul>
                        
                <li>
                    <a href="#k-fold-cross-validation" aria-label="K-fold Cross Validation">K-fold Cross Validation</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="how-does-knn-algorithm-work">How does KNN algorithm work?<a hidden class="anchor" aria-hidden="true" href="#how-does-knn-algorithm-work">#</a></h1>
<p>The letter &ldquo;K&rdquo; in KNN means <code>the number of neighbors  </code> around the test sample.  During prediction it searches for the nearest neighbors and <strong>takes their majority vote as the class predicted for the sample</strong>.</p>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">flowchart LR
	id1(For a sample) --&gt; id2(Find its k nearest neighbor) --&gt; id3(Take majority vote as sample class predicted for the sample)
</code></pre><h1 id="find-neighbor-distancesimilarity-metric-some-norms">Find neighbor: distance/similarity metric (some norms)<a hidden class="anchor" aria-hidden="true" href="#find-neighbor-distancesimilarity-metric-some-norms">#</a></h1>
<h3 id="minikowski-distance">Minikowski distance<a hidden class="anchor" aria-hidden="true" href="#minikowski-distance">#</a></h3>
<p>$$
D(X, Y)=\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{p}\right)^{\frac{1}{p}}
$$</p>
<h3 id="norm-l_0">(Norm $l_0$)<a hidden class="anchor" aria-hidden="true" href="#norm-l_0">#</a></h3>
<h3 id="matthan-distrance-norm-l_1-p1">Matthan distrance (Norm $l_1$, $p=1$)<a hidden class="anchor" aria-hidden="true" href="#matthan-distrance-norm-l_1-p1">#</a></h3>
<p>$$
D(X, Y)=\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|\right)
$$</p>
<h3 id="euclidean-distance-norm-l_2-p2">Euclidean distance (Norm $l_2$, $p=2$)<a hidden class="anchor" aria-hidden="true" href="#euclidean-distance-norm-l_2-p2">#</a></h3>
<p>$$
D(X, Y)=\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{2}\right)^{\frac{1}{2}}
$$</p>
<h3 id="l-infinity-distance-norm-l_infty">L-infinity distance (Norm $l_\infty$)<a hidden class="anchor" aria-hidden="true" href="#l-infinity-distance-norm-l_infty">#</a></h3>
<p>$$
D(X, Y)=\text{max}\left|x_{i}-y_{i}\right|
$$</p>
<h3 id="l-negative-infinity-distance-norm-l_infty">L-negative-infinity distance (Norm $l_\infty$)<a hidden class="anchor" aria-hidden="true" href="#l-negative-infinity-distance-norm-l_infty">#</a></h3>
<p>$$
D(X, Y)=\text{min}\left|x_{i}-y_{i}\right|
$$</p>
<h1 id="feature-scaling---normalization">Feature scaling - Normalization<a hidden class="anchor" aria-hidden="true" href="#feature-scaling---normalization">#</a></h1>
<p><strong>Why Normalization?</strong></p>
<blockquote>
<p>To avoid bias towards variables with higher magnitude</p>
</blockquote>
<h3 id="standard-score">Standard score<a hidden class="anchor" aria-hidden="true" href="#standard-score">#</a></h3>
<p>$$
f_{i} \leftarrow \frac{f_{i}-\mu_{i}}{\sigma_{i}}
$$</p>
<ul>
<li>
<p>Represents the feature value in terms of $\sigma$ units from mean</p>
</li>
<li>
<p>Works well for populations that are normally distributed</p>
</li>
</ul>
<h3 id="min-max-feature-scaling">Min-max feature scaling<a hidden class="anchor" aria-hidden="true" href="#min-max-feature-scaling">#</a></h3>
<p>$$
f_{i} \leftarrow \frac{f_{i}-f_{\min }}{f_{\max }-f_{\min }}
$$</p>
<ul>
<li>Set all feature values within [0,1] range</li>
</ul>
<h1 id="three-knn-algorithms-brute-force-ball-tree-and-k-d-tree">Three KNN algorithms: Brute force, Ball tree, and k-d tree<a hidden class="anchor" aria-hidden="true" href="#three-knn-algorithms-brute-force-ball-tree-and-k-d-tree">#</a></h1>
<h3 id="brute-force-method">Brute force method<a hidden class="anchor" aria-hidden="true" href="#brute-force-method">#</a></h3>
<p>Training time complexity:  $O(1)$</p>
<p>Training space complexity:  $O(1)$</p>
<p>Prediction time complexity:  $O(k<em>n</em>d)$</p>
<p>​	each sample, calculate <code>d</code> times to get distance, then we have <code>n</code> sample, thus <code>n*d</code>, finally we have <code>k</code> samples need to be found, hence $k<em>n</em>d$.</p>
<p>Prediction space complexity: $O(1)$</p>
<p>Training phase technically <strong>does not exist</strong>, since all <strong>computation is done during prediction</strong>, so we have O(1) for both time and space.</p>
<p>Prediction phase is, as method name suggest, a simple exhaustive search, which in pseudocode is:</p>
<p>Loop through all points k times:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-markdown" data-lang="markdown"><span style="color:#66d9ef">1.</span> Compute the distance between currently classifier sample and training points, remember the index of the element with the smallest distance (ignore previously selected points)
<span style="color:#66d9ef">2.</span> Add the class at found index to the counter 
Return the class with the most votes as a prediction
</code></pre></div><p>This is a nested loop structure, where the outer loop takes k steps and the inner loop takes n steps. 3rd point is $O(1$) and 4th is $O(\text{number of classes})$, so they are smaller. Additionally, we have to take into consideration the numer of dimensions d, more directions mean longer vectors to compute distances. Therefore, we have $O(n * k * d)$ time complexity.</p>
<p>As for space complexity, we need a small vector to count the votes for each class. It’s almost always very small and is fixed, so we can treat it as a O(1) space complexity.</p>
<h3 id="ball-tree-method">Ball tree method<a hidden class="anchor" aria-hidden="true" href="#ball-tree-method">#</a></h3>
<p>Training time complexity: $O(d * n * log(n))$</p>
<p>Training space complexity: $O(d * n)$</p>
<p>Prediction time complexity: $O(k * log(n))$</p>
<p>Prediction space complexity: $O(1)$</p>
<p>Ball tree algorithm takes another approach to dividing space where training points lie. In contrast to k-d trees, which divides space with median value “cuts”, ball tree groups points into “balls” organized into a tree structure. They go from the largest (root, with all points) to the smallest (leaves, with only a few or even 1 point). It allows fast nearest neighbor lookup because nearby neighbors are in the same or at least close “balls”.</p>
<p>During the training phase, we only need to construct the ball tree. There are a few algorithms for constructing the ball tree, but the one most similar to k-d tree (called “k-d construction algorithm” for that reason) is $O(d * n * log(n))$, the same as k-d tree.</p>
<p>Because of the tree building similarity, the complexities of the prediction phase are also the same as for k-d tree.</p>
<h3 id="k-d-tree-method">k-d tree method<a hidden class="anchor" aria-hidden="true" href="#k-d-tree-method">#</a></h3>
<p>Check a demonstration video <a href="https://www.youtube.com/watch?v=BK5x7IUTIyU">here</a>.</p>
<p>Training time complexity: $O(d * n * log(n))$</p>
<p>Training space complexity: $O(d * n)$</p>
<p>Prediction time complexity: $O(k * log(n))$</p>
<p>Prediction space complexity: $O(1)$</p>
<p>During the training phase, we have to construct the k-d tree. This data structure splits the k-dimensional space (here k means k dimensions of space, don’t confuse this with k as a number of nearest neighbors!) and allows faster search for nearest points, since we “know where to look” in that space. You may think of it like a generalization of BST for many dimensions. It “cuts” space with axis-aligned cuts, dividing points into groups in children nodes.</p>
<p>Constructing the k-d tree is not a machine learning task itself, since it stems from computational geometry domain, so we won’t cover this in detail, only on conceptual level. The time complexity is usually $O(d * n * log(n))$, because insertion is $O(log(n))$ (similar to <a href="https://www.youtube.com/watch?v=-5LJy9htres">regular BST</a>) and we have n points from the training dataset, each with d dimensions. I assume the efficient implementation of the data structure, i. e. it finds the optimal split point (median in the dimension) in O(n), which is possible with the median of medians algorithm. Space complexity is $O(d * n)$ — note that it depends on dimensionality d, which makes sense, since more dimensions correspond to more space divisions and larger trees (in addition to larger time complexity for the same reason).</p>
<p>As for the prediction phase, the k-d tree structure naturally supports “k nearest point neighbors query” operation, which is exactly what we need for kNN. The simple approach is to just query k times, removing the point found each time — since query takes $O(log(n))$, it is $O(k * log(n))$ in total. But since the k-d tree already cuts space during construction, after a single query we approximately know where to look — we can just search the “surroundings” around that point. Therefore, practical implementations of k-d tree support querying for whole k neighbors at one time and with complexity $O(sqrt(n) + k)$, which is much better for larger dimensionalities, which are very common in machine learning.</p>
<p>The above complexities are the average ones, assuming the balanced k-d tree. The O(log(n)) times assumed above may degrade up to $O(n)$ for unbalanced trees, but if the median is used during the tree construction, we should always get a tree with approximately $O(log(n))$ insertion/deletion/search complexity.</p>
<h3 id="choosing-the-method-in-practice">Choosing the method in practice<a hidden class="anchor" aria-hidden="true" href="#choosing-the-method-in-practice">#</a></h3>
<p>To summarize the complexities: brute force is the <strong>slowest</strong> in the big O notation, while both k-d tree and ball tree have the same lower complexity. How do we know which one to use then?</p>
<p>To get the answer, we have to look at both training and prediction times, that’s why I have provided both. The brute force algorithm has only one complexity, for prediction, $O(k * n)$. Other algorithms need to create the data structure first, so for training and prediction they get $O(d * n * log(n) + k * log(n))$, not taking into account the space complexity, which may also be important. Therefore, where the construction of the trees is frequent, the training phase may outweigh their advantage of faster nearest neighbor lookup.</p>
<p>Should we use k-d tree or ball tree? It depends on the data structure — relatively uniform or “well behaved” data will make better use of k-d tree, since the cuts of space will work well (near points will be close in the leaves after all cuts). For more clustered data the “balls” from the ball tree will reflect the structure better and therefore allow for faster nearest neighbor search. Fortunately, Scikit-learn supports “auto” option, which will automatically infer the best data structure from the data.</p>
<p>Let’s see this in practice on two case studies, which I’ve encountered in practice during my studies and job.</p>
<h2 id="case-study-1-classification">Case study 1: classification<a hidden class="anchor" aria-hidden="true" href="#case-study-1-classification">#</a></h2>
<p>The more “traditional” application of the kNN is the classification of data. It often has quite a lot of points, e. g. MNIST has 60k training images and 10k test images. Classification is done offline, which means we first do the train ing phase, then just use the results during prediction. Therefore, if we want to construct the data structure, we only need to do so once. For 10k test images, let’s compare the brute force (which calculates all distances every time) and k-d tree for 3 neighbors:</p>
<p>Brute force $(O(k * n))$: 3 * 10,000 = 30,000</p>
<p>k-d tree $ (O(k * log(n)))$: 3 * log(10,000) ~ 3 * 13 = 39</p>
<p>Comparison: 39 / 30,000 = 0.0013</p>
<p>As you can see, the performance gain is huge! The data structure method uses only a tiny fraction of the brute force time. For most datasets this method is a clear winner.</p>
<h2 id="case-study-2-real-time-smart-monitoring">Case study 2: real-time smart monitoring<a hidden class="anchor" aria-hidden="true" href="#case-study-2-real-time-smart-monitoring">#</a></h2>
<p>Machine Learning is commonly used for image recognition, often using neural networks. It’s very useful for real-time applications, where it’s often integrated with cameras, alarms etc. The problem with neural networks is that they often <strong>detect the same object 2 or more times</strong> — even the best architectures like YOLO have this problem. We can actually solve it with nearest neighbor search with a simple approach:</p>
<ul>
<li>
<p>Calculate the center of each bounding box (rectangle)</p>
</li>
<li>
<p>For each rectangle, search for its nearest neighbor (1NN)</p>
</li>
<li>
<p>If points are closer than the selected threshold, merge them (they detect the same object)</p>
</li>
</ul>
<p>The crucial part is searching for the closest center of another bounding box (point 2). Which algorithm should be used here? Typically we have only a few moving objects on camera, maybe up to 30–40. For such a small number, speedup from using data structures for faster lookup is negligible. Each frame is a separate image, so if we wanted to construct a k-d tree for example, we would have to do so for every frame, which may mean 30 times per second — a huge cost overall. Therefore, for such situations a simple brute force method works fastest and also has the smallest space requirement (which, with heavy neural networks or for embedded CPUs in cameras, may be important).</p>
<h1 id="how-to-choose-a-proper-k">How to choose a proper k?<a hidden class="anchor" aria-hidden="true" href="#how-to-choose-a-proper-k">#</a></h1>
<h2 id="k-fold-cross-validation">K-fold Cross Validation<a hidden class="anchor" aria-hidden="true" href="#k-fold-cross-validation">#</a></h2>
<blockquote>
<p>Cross-validation is a statistical method used to estimate the skill of machine learning models.</p>
</blockquote>
<ul>
<li>
<p>Advantage: Can avoid overfitting and under-fitting</p>
</li>
<li>
<p>Disadvantage: K should be large enough</p>
</li>
</ul>
<p>Reference</p>
<p><a href="https://www.youtube.com/watch?v=twf-Fjq7Lz4">For complexity explanation</a></p>
<p><a href="https://towardsdatascience.com/k-nearest-neighbors-computational-complexity-502d2c440d5">For three KNN algorithms</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="/tags/data-science/">Data Science</a></li>
      <li><a href="/tags/learning-notes/">learning notes</a></li>
      <li><a href="/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="/tags/algorithm/">Algorithm</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="/">Landisland</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>

    <br>
    <span>All Conetents Are Licensed Under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nooopener noreferrer" target="_blank">CC BY-NC 4.0</a> Unless Otherwise Stated</span>
    <br>
    

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
