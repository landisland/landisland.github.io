<!DOCTYPE html>
<html lang="en" dir="auto">

<head>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Linear model selection and regularization | Landisland</title>
<meta name="keywords" content="tatistics" />
<meta name="description" content="Summary about comparison between least square method and shrinkage method.">
<meta name="author" content="">
<link rel="canonical" href="/post/2022-02-27linear-model-selection-and-regularization/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css" integrity="sha256-yIlj/i15RiAA/Q&#43;xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="mask-icon" href="/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.91.2" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Linear model selection and regularization" />
<meta property="og:description" content="Summary about comparison between least square method and shrinkage method." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/2022-02-27linear-model-selection-and-regularization/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-02-27T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-02-27T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Linear model selection and regularization"/>
<meta name="twitter:description" content="Summary about comparison between least square method and shrinkage method."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "/post/"
    }
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Linear model selection and regularization",
      "item": "/post/2022-02-27linear-model-selection-and-regularization/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Linear model selection and regularization",
  "name": "Linear model selection and regularization",
  "description": "Summary about comparison between least square method and shrinkage method.",
  "keywords": [
    "tatistics"
  ],
  "articleBody": "Some take out:\n If $p \\ge n$, least square method may have high variance, hence we use shrinkage method which can reduce variance.  1. Problem with linear regression with least square (ls)  Prediction Accuracy: linear regression has low bias but suffer from high variance, especially when $n \\approx p$. It cannot handle $n Model Interpretability: It is often the case that some or many of the variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to unnecessary complexity in the resulting model.  2. Selected alternatives to LS  Subset Selection. This approach involves identifying a subset of the $p$ predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables. Shrinkage. This approach involves fitting a model involving all $p$ predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing variance. Depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero. Hence, shrinkage methods can also perform variable selection.  Ridge Regression (Norm 2) Lasso (Norm 1) can estimate coefficients to zero   Dimension Reduction. This approach involves projecting the $p$ predictors into a $M$-dimensional subspace, where $MBest subset selection  Let $\\mathcal{M}_{0}$ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.  For $k=1,2, \\ldots p$ : (a) Fit all $\\left(\\begin{array}{l}p \\ k\\end{array}\\right)$ models that contain exactly $k$ predictors. (b) Pick the best among these $\\left(\\begin{array}{l}p \\ k\\end{array}\\right)$ models, and call it $\\mathcal{M}_{k}$. Here best is defined as having the smallest RSS, or equivalently largest $R^{2}$. Select a single best model from among $\\mathcal{M}{0}, \\ldots, \\mathcal{M}{p}$ using crossvalidated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^{2}$.   Forward stepwise selection and Backward stepwise selection  Forward stepwise selection (can be used even $n \\le p$)  Let $\\mathcal{M}_{0}$ denote the null model, which contains no predictors.  For $k=0, \\ldots, p-1$ : (a) Consider all $p-k$ models that augment the predictors in $\\mathcal{M}{k}$ with one additional predictor. (b) Choose the best among these $p-k$ models, and call it $\\mathcal{M}{k+1}$. Here best is defined as having smallest RSS or highest $R^{2}$. Select a single best model from among $\\mathcal{M}{0}, \\ldots, \\mathcal{M}{p}$ using crossvalidated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^{2}$.   Backward stepwise selection (require $n \\ge p$)  Let $\\mathcal{M}_{p}$ denote the full model, which contains all $p$ predictors. For $k=p, p-1, \\ldots, 1$ : (a) Consider all $k$ models that contain all but one of the predictors in $\\mathcal{M}{k}$, for a total of $k-1$ predictors. (b) Choose the best among these $k$ models, and call it $\\mathcal{M}{k-1}$. Here best is defined as having smallest RSS or highest $R^{2}$. Select a single best model from among $\\mathcal{M}{0}, \\ldots, \\mathcal{M}{p}$ using crossvalidated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^{2}$.   $$\\begin{aligned} \u0026C_{p}=\\frac{1}{n}\\left(\\mathrm{RSS}+2 d \\hat{\\sigma}^{2}\\right) \\ \u0026\\mathrm{AIC}=\\frac{1}{n \\hat{\\sigma}^{2}}\\left(\\mathrm{RSS}+2 d \\hat{\\sigma}^{2}\\right) \\ \u0026\\mathrm{BIC}=\\frac{1}{n}\\left(\\mathrm{RSS}+\\log (n) d \\hat{\\sigma}^{2}\\right) \\ \u0026\\text { Adjusted } R^{2}=1-\\frac{\\mathrm{RSS} /(n-d-1)}{\\mathrm{TSS} /(n-1)} \\end{aligned}$$      3. Shrinkage in details Ridge regression Ridge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. In particular, the ridge regression coefficient estimates $\\beta^{R}$ are the values that minimize $$ \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}=\\mathrm{RSS}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2} $$ where $\\lambda \\geq 0$ is a tuning parameter, to be determined separately. The above equation trades off two different criteria. As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the $RSS$ small. However, the second term, $\\lambda \\sum_{j} \\beta_{j}^{2}$, called a shrinkage penalty, is small when $\\beta_{1}, \\ldots, \\beta_{p}$ are close to zero, and so it has the effect of shrinking penalty the estimates of $\\beta_{j}$ towards zero.\nUnlike least squares, which generates only one set of coefficient estimates, ridge regression will produce a different set of coefficient estimates, $\\hat{\\beta}_{\\lambda}^{R}$, for each value of $\\lambda$. Selecting a good value for $\\lambda$ is critical.\nWe want to shrink the estimated association of each variable with the response; however, we do not want to shrink the intercept, which is simply a measure of the mean value of the response when $x_{i 1}=x_{i 2}=\\ldots=x_{i p}=0$. If we assume that the variables-that is, the columns of the data matrix $X$-have been centered to have mean zero before ridge regression is performed, then the estimated intercept will take the form $\\hat{\\beta}_{0}=\\bar{y}$. The shrinkage penalty is not scale invariant. Therefore, it is best to apply ridge regression after standardizing the predictors.\n$\\lambda$ Increases will lead to decrease of flexibility hence lower variance and higher bias.\nIn general, in situations where the relationship between the response and the predictors is close to linear, the least squares estimates will have low bias but may have high variance. This means that a small change in the training data can cause a large change in the least squares coefficient estimates. In particular, when the number of variables $p$ is almost as large as the number of observations $n$, the least squares estimates will be extremely variable. And if $pn$, then the least squares estimates do not even have a unique solution, whereas ridge regression can still perform well by trading off a small increase in bias for a large decrease in variance. Hence, ridge regression works best in situations where the least squares estimates have high variance.\nRidge regression compared to subset selection Ridge regression also has substantial computational advantages over best subset selection, which requires searching through $2^{p}$ models. As we discussed previously, even for moderate values of $p$, such a search can be computationally infeasible. In contrast, for any fixed value of $\\lambda$, ridge regression only fits a single model, and the model-fitting procedure can be performed quite quickly. In fact, one can show that the computations required to solve the penalized least square, simultaneously for all values of $\\lambda$, are almost identical to those for fitting a model using least squares.\n",
  "wordCount" : "1033",
  "inLanguage": "en",
  "datePublished": "2022-02-27T00:00:00Z",
  "dateModified": "2022-02-27T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/2022-02-27linear-model-selection-and-regularization/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Landisland",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="/" accesskey="h" title="Landisland (Alt + H)">Landisland</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="/about" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
    integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
    integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
    crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
    integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>



<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Linear model selection and regularization
    </h1>
    <div class="post-meta"><span title='2022-02-27 00:00:00 +0000 UTC'>February 27, 2022</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-problem-with-linear-regression-with-least-square-ls" aria-label="1. Problem with linear regression with least square (ls)">1. Problem with linear regression with least square (ls)</a></li>
                <li>
                    <a href="#2-selected-alternatives-to-ls" aria-label="2. Selected alternatives to LS">2. Selected alternatives to LS</a></li>
                <li>
                    <a href="#3-shrinkage-in-details" aria-label="3. Shrinkage in details">3. Shrinkage in details</a><ul>
                        
                <li>
                    <a href="#ridge-regression" aria-label="Ridge regression">Ridge regression</a></li>
                <li>
                    <a href="#ridge-regression-compared-to-subset-selection" aria-label="Ridge regression compared to subset selection">Ridge regression compared to subset selection</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Some take out:</p>
<ul>
<li>If $p \ge n$, least square method may have high variance, hence we use shrinkage method which can reduce variance.</li>
</ul>
<h2 id="1-problem-with-linear-regression-with-least-square-ls">1. Problem with linear regression with least square (ls)<a hidden class="anchor" aria-hidden="true" href="#1-problem-with-linear-regression-with-least-square-ls">#</a></h2>
<ul>
<li>Prediction Accuracy: linear regression has <strong>low bias</strong> but suffer from <strong>high variance</strong>, especially when $n \approx p$. It cannot handle $n&lt;p$.</li>
<li>Model Interpretability: It is often the case that some or many of the variables used in a multiple regression model are <strong>in fact not associated with the response</strong>. Including such irrelevant variables leads to unnecessary complexity in the resulting model.</li>
</ul>
<h2 id="2-selected-alternatives-to-ls">2. Selected alternatives to LS<a hidden class="anchor" aria-hidden="true" href="#2-selected-alternatives-to-ls">#</a></h2>
<ul>
<li><strong>Subset Selection</strong>. This approach involves identifying <strong>a subset of the $p$ predictors</strong> that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.</li>
<li><strong>Shrinkage</strong>. This approach involves fitting a model involving all $p$ predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of <strong>reducing variance.</strong>
Depending on what type of shrinkage is performed, <strong>some of the coefficients may be estimated to be exactly zero</strong>. Hence, shrinkage methods can also perform variable selection.
<ul>
<li>Ridge Regression (Norm 2)</li>
<li>Lasso (Norm 1)  can estimate coefficients to zero</li>
</ul>
</li>
<li><strong>Dimension Reduction</strong>. This approach involves projecting the $p$ predictors into a $M$-dimensional subspace, where $M&lt;\mathrm{p}$. This is achieved by computing $M$ different linear combinations, or projections, of the variables. Then these $M$ projections are used as predictors to fit a linear regression model by least squares
<ul>
<li><strong>Best subset selection</strong>
<ul>
<li>Let $\mathcal{M}_{0}$ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.</li>
</ul>
<ol start="2">
<li>For $k=1,2, \ldots p$ :
(a) Fit all $\left(\begin{array}{l}p \ k\end{array}\right)$ models that contain exactly $k$ predictors.
(b) Pick the best among these $\left(\begin{array}{l}p \ k\end{array}\right)$ models, and call it $\mathcal{M}_{k}$. Here best is defined as having the smallest RSS, or equivalently largest $R^{2}$.</li>
<li>Select a single best model from among $\mathcal{M}<em>{0}, \ldots, \mathcal{M}</em>{p}$ using crossvalidated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^{2}$.</li>
</ol>
</li>
<li><strong>Forward stepwise selection</strong> and  <strong>Backward stepwise selection</strong>
<ul>
<li>Forward stepwise selection (can be used even $n \le p$)
<ul>
<li>Let $\mathcal{M}_{0}$ denote the null model, which contains no predictors.</li>
</ul>
<ol start="2">
<li>For $k=0, \ldots, p-1$ :
(a) Consider all $p-k$ models that augment the predictors in $\mathcal{M}<em>{k}$ with one additional predictor.
(b) Choose the best among these $p-k$ models, and call it $\mathcal{M}</em>{k+1}$. Here best is defined as having smallest RSS or highest $R^{2}$.</li>
<li>Select a single best model from among $\mathcal{M}<em>{0}, \ldots, \mathcal{M}</em>{p}$ using crossvalidated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^{2}$.</li>
</ol>
</li>
<li>Backward stepwise selection (require $n \ge p$)
<ul>
<li>Let $\mathcal{M}_{p}$ denote the full model, which contains all $p$ predictors.</li>
<li>For $k=p, p-1, \ldots, 1$ :
(a) Consider all $k$ models that contain all but one of the predictors in $\mathcal{M}<em>{k}$, for a total of $k-1$ predictors.
(b) Choose the best among these $k$ models, and call it $\mathcal{M}</em>{k-1}$. Here best is defined as having smallest RSS or highest $R^{2}$.</li>
<li>Select a single best model from among $\mathcal{M}<em>{0}, \ldots, \mathcal{M}</em>{p}$ using crossvalidated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^{2}$.</li>
</ul>
</li>
<li>$$\begin{aligned}
&amp;C_{p}=\frac{1}{n}\left(\mathrm{RSS}+2 d \hat{\sigma}^{2}\right) \
&amp;\mathrm{AIC}=\frac{1}{n \hat{\sigma}^{2}}\left(\mathrm{RSS}+2 d \hat{\sigma}^{2}\right) \
&amp;\mathrm{BIC}=\frac{1}{n}\left(\mathrm{RSS}+\log (n) d \hat{\sigma}^{2}\right) \
&amp;\text { Adjusted } R^{2}=1-\frac{\mathrm{RSS} /(n-d-1)}{\mathrm{TSS} /(n-1)}
\end{aligned}$$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="3-shrinkage-in-details">3. Shrinkage in details<a hidden class="anchor" aria-hidden="true" href="#3-shrinkage-in-details">#</a></h2>
<h3 id="ridge-regression">Ridge regression<a hidden class="anchor" aria-hidden="true" href="#ridge-regression">#</a></h3>
<p>Ridge regression is very similar to least squares, except that the <strong>coefficients are estimated by minimizing a slightly different quantity</strong>. In particular, the ridge regression coefficient estimates $\beta^{R}$ are the values that minimize
$$
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}=\mathrm{RSS}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}
$$
where $\lambda \geq 0$ is a <strong>tuning parameter</strong>, to be determined separately. The above equation trades off two different criteria. As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the $RSS$ small. However, the second term, $\lambda \sum_{j} \beta_{j}^{2}$, called a <strong>shrinkage penalty</strong>, is small when $\beta_{1}, \ldots, \beta_{p}$ are close to zero, and so it has the effect of shrinking penalty the estimates of $\beta_{j}$ towards zero.</p>
<p>Unlike least squares, which generates <strong>only one set</strong> of coefficient estimates, ridge regression will produce a different set of coefficient estimates, $\hat{\beta}_{\lambda}^{R}$, for each value of $\lambda$. <strong>Selecting a good value for $\lambda$ is critical.</strong></p>
<p>We want to shrink the estimated association of each variable with the response; however, <strong>we do not want to shrink the intercept</strong>, which is simply a measure of the mean value of the response when $x_{i 1}=x_{i 2}=\ldots=x_{i p}=0$. If we assume that the variables-that is, the columns of the data matrix $X$-have been centered to have mean zero before ridge regression is performed, then the estimated intercept will take the form $\hat{\beta}_{0}=\bar{y}$.
The shrinkage penalty is not <strong>scale invariant</strong>. Therefore, it is best to <strong>apply ridge regression after standardizing the predictors.</strong></p>
<p><strong>$\lambda$ Increases will lead to decrease of flexibility hence lower variance and higher bias.</strong></p>
<p>In general, in situations where the relationship between the response and the predictors is <strong>close to linear</strong>, the least squares estimates will have <strong>low bias but may have high variance</strong>. <strong>This means that a small change in the training data can cause a large change in the least squares coefficient estimates.</strong> In particular, when the number of variables $p$ is almost as large as the number of observations $n$, the least squares estimates will be extremely variable. <strong>And if $p&gt;n$, then the least squares estimates do not even have a unique solution,</strong> whereas ridge regression can still perform well by trading off a small increase in bias for a large decrease in variance. <strong>Hence, ridge regression works best in situations where the least squares estimates have high variance.</strong></p>
<h3 id="ridge-regression-compared-to-subset-selection">Ridge regression compared to subset selection<a hidden class="anchor" aria-hidden="true" href="#ridge-regression-compared-to-subset-selection">#</a></h3>
<p>Ridge regression also has <strong>substantial computational advantages</strong> over best subset selection, which requires searching through $2^{p}$ models. As we discussed previously, even for moderate values of $p$, such a search can be computationally infeasible. In contrast, for any fixed value of $\lambda$, ridge regression only fits a single model, and the model-fitting procedure can be performed quite quickly. In fact, one can show that the computations required to solve the penalized least square, simultaneously for all values of $\lambda$, are almost identical to those for fitting a model using least squares.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="/tags/tatistics/">tatistics</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="/">Landisland</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>

    <br>
    <span>All Conetents Are Licensed Under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nooopener noreferrer" target="_blank">CC BY-NC 4.0</a> Unless Otherwise Stated</span>
    <br>
    

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
