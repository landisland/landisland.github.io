<!DOCTYPE html>
<html lang="en" dir="auto">

<head>

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Some Cross-Validation Methods | Landisland</title>
<meta name="keywords" content="Cross Validation" />
<meta name="description" content="Summary about some cross-validation methods and their pros and cons">
<meta name="author" content="">
<link rel="canonical" href="/post/2022-03-02some-cross-validation-methods/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css" integrity="sha256-yIlj/i15RiAA/Q&#43;xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="mask-icon" href="/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.91.2" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Some Cross-Validation Methods" />
<meta property="og:description" content="Summary about some cross-validation methods and their pros and cons" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/2022-03-02some-cross-validation-methods/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-03-02T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-03-02T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Some Cross-Validation Methods"/>
<meta name="twitter:description" content="Summary about some cross-validation methods and their pros and cons"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "/post/"
    }
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Some Cross-Validation Methods",
      "item": "/post/2022-03-02some-cross-validation-methods/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Some Cross-Validation Methods",
  "name": "Some Cross-Validation Methods",
  "description": "Summary about some cross-validation methods and their pros and cons",
  "keywords": [
    "Cross Validation"
  ],
  "articleBody": "Why Cross-Validation is Important? We often randomly split the dataset into train data and test data to develop a machine learning model. The training data is used to train the ML model and the same model is tested on independent testing data to evaluate the performance of the model.\nWith the change in the random state of the split, the accuracy of the model also changes, so we are not able to achieve a fixed accuracy for the model. The testing data should be kept independent of the training data so that no data leakage occurs. During the development of an ML model using the training data, the model performance needs to be evaluated. Hereâ€™s the importance of cross-validation data comes into the picture.\nLeave-one-out cross-validation (LOOCV) It is a category of LpOCV with the case of p=1.\nFor a dataset having $n$ rows, 1 st row is selected for validation, and the rest (n-1) rows are used to train the model. For the next iteration, the 2 nd row is selected for validation and rest to train the model. Similarly, the process is repeated until $\\mathrm{n}$ steps or the desired number of operations.\nBoth the above two cross-validation techniques are the types of exhaustive cross-validation. Exhaustive crossvalidation methods are cross-validation methods that learn and test in all possible ways. They have the same pros and cons discussed below:\nPros:\n Simple, easy to understand, and implement.  Cons:\n The model may lead to a low bias. The computation time required is high.  Holdout cross-validation The holdout cross-validation randomly splits the dataset into train and test data depending on data analysis.\nIn the case of holdout cross-validation, the dataset is randomly split into training and validation data. Generally, the split of training data is more than test data. The training data is used to induce the model and validation data is evaluates the performance of the model. The more data is used to train the model, the better the model is. For the holdout cross-validation method, a good amount of data is isolated from training.\nPros:\n Same as previous.  Cons:\n Not suitable for an imbalanced dataset. A lot of data is isolated from training the model.  k-fold cross-validation In k-fold cross-validation, the original dataset is equally partitioned into $\\mathrm{k}$ subparts or folds. Out of the k-folds or groups, for each iteration, one group is selected as validation data, and the remaining $(\\mathrm{k}-1)$ groups are selected as training data.\nThe process is repeated for $\\mathrm{k}$ times until each group is treated as validation and remaining as training data.\nThe final accuracy of the model is computed by taking the mean accuracy of the k-models validation data. $$ \\mathbf{a c c}{\\mathrm{cv}}=\\sum{\\mathbf{i}=1}^{\\mathrm{k}} \\frac{\\mathbf{a c c}_{\\mathrm{i}}}{\\mathbf{k}} $$ Pros:\n The model has low bias Low time complexity The entire dataset is utilized for both training and validation.  Cons:\n Not suitable for an imbalanced dataset.  Repeated random subsampling validation Repeated random subsampling validation also referred to as Monte Carlo crossvalidation splits the dataset randomly into training and validation. Unlikely k-fold cross-validation split of the dataset into not in groups or folds but splits in this case in random.\nThe number of iterations is not fixed and decided by analysis. The results are then averaged over the splits. $$ \\mathbf{a c c}{\\mathrm{cv}}=\\sum{\\mathbf{i}=1}^{\\mathrm{k}} \\frac{\\mathbf{a c c}_{\\mathrm{i}}}{\\mathbf{k}} $$ Pros:\n The proportion of train and validation splits is not dependent on the number of iterations or partitions.  Cons:\n Some samples may not be selected for either training or validation. Not suitable for an imbalanced dataset.  Stratified k-fold cross-validation For all the cross-validation techniques discussed above, they may not work well with an imbalanced dataset. Stratified kfold cross-validation solved the problem of an imbalanced dataset.\nIn Stratified k-fold cross-validation, the dataset is partitioned into $\\mathrm{k}$ groups or folds such that the validation data has an equal number of instances of target class label. This ensures that one particular class is not over present in the validation or train data especially when the dataset is imbalanced.\nThe final score is computed by taking the mean of scores of each fold.\nPros:\n Works well for an imbalanced dataset.  Cons:\n Now suitable for time series dataset.  Time Series cross-validation The order of the data is very important for time-series related problem. For timerelated dataset random split or k-fold split of data into train and validation may not yield good results.\nFor the time-series dataset, the split of data into train and validation is according to the time also referred to as forward chaining method or rolling cross-validation. For a particular iteration, the next instance of train data can be treated as validation data.\nNested cross-validation In the case of k-fold and stratified k-fold cross-validation, we get a poor estimate of the error in training and test data.\nHyperparameter tuning is done separately in the earlier methods. When cross- validation is used simultaneously for tuning the hyperparameters and generalizing the error estimate, nested cross-validation is required.\nNested Cross Validation can be applicable in both $k$-fold and stratified $k$-fold variants.\nConclusion Cross-validation is used to compare and evaluate the performance of ML models. In this article, we have covered 8 crossvalidation techniques along with their pros and cons. k-fold and stratified k-fold crossvalidations are the most used techniques. Time series cross-validation works best with time series related problems.\nReference\n",
  "wordCount" : "881",
  "inLanguage": "en",
  "datePublished": "2022-03-02T00:00:00Z",
  "dateModified": "2022-03-02T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/2022-03-02some-cross-validation-methods/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Landisland",
    "logo": {
      "@type": "ImageObject",
      "url": "/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="/" accesskey="h" title="Landisland (Alt + H)">Landisland</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://landisland.zhubai.love/" title="Newsletter(Chinese)">
                    <span>Newsletter(Chinese)</span>
                </a>
            </li>
            <li>
                <a href="/tags" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="/about" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
    integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
    integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
    crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
    integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>



<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Some Cross-Validation Methods
    </h1>
    <div class="post-meta"><span title='2022-03-02 00:00:00 +0000 UTC'>March 2, 2022</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#why-cross-validation-is-important" aria-label="Why Cross-Validation is Important?">Why Cross-Validation is Important?</a></li>
                <li>
                    <a href="#leave-one-out-cross-validation-loocv" aria-label="Leave-one-out cross-validation (LOOCV)">Leave-one-out cross-validation (LOOCV)</a></li>
                <li>
                    <a href="#holdout-cross-validation" aria-label="Holdout cross-validation">Holdout cross-validation</a></li>
                <li>
                    <a href="#k-fold-cross-validation" aria-label="k-fold cross-validation">k-fold cross-validation</a></li>
                <li>
                    <a href="#repeated-random-subsampling-validation" aria-label="Repeated random subsampling validation">Repeated random subsampling validation</a></li>
                <li>
                    <a href="#stratified-k-fold-cross-validation" aria-label="Stratified k-fold cross-validation">Stratified k-fold cross-validation</a></li>
                <li>
                    <a href="#time-series-cross-validation" aria-label="Time Series cross-validation">Time Series cross-validation</a></li>
                <li>
                    <a href="#nested-cross-validation" aria-label="Nested cross-validation">Nested cross-validation</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="why-cross-validation-is-important">Why Cross-Validation is Important?<a hidden class="anchor" aria-hidden="true" href="#why-cross-validation-is-important">#</a></h2>
<p>We often randomly split the dataset into train data and test data to develop a machine learning model. The training data is used to train the ML model and the same model is tested on independent testing data to evaluate the performance of the model.</p>
<p>With the change in the random state of the split, the accuracy of the model also changes, so we are not able to achieve a fixed accuracy for the model. The testing data should be kept independent of the training data so that no data leakage occurs. During the development of an ML model using the training data, the model performance needs to be evaluated. Here&rsquo;s the importance of cross-validation data comes into the picture.</p>
<h2 id="leave-one-out-cross-validation-loocv">Leave-one-out cross-validation (LOOCV)<a hidden class="anchor" aria-hidden="true" href="#leave-one-out-cross-validation-loocv">#</a></h2>
<p>It is a category of LpOCV with the case of p=1.</p>
<p>For a dataset having $n$ rows, 1 st row is selected for validation, and the rest (n-1) rows are used to train the model. For the next iteration, the 2 nd row is selected for validation and rest to train the model. Similarly, the process is repeated until $\mathrm{n}$ steps or the desired number of operations.</p>
<p>Both the above two cross-validation techniques are the types of exhaustive cross-validation. Exhaustive crossvalidation methods are cross-validation methods that learn and test in all possible ways. They have the same pros and cons discussed below:</p>
<p><strong>Pros:</strong></p>
<ol>
<li>Simple, easy to understand, and implement.</li>
</ol>
<p><strong>Cons:</strong></p>
<ol>
<li>The model may lead to a low bias.</li>
<li>The computation time required is high.</li>
</ol>
<h2 id="holdout-cross-validation">Holdout cross-validation<a hidden class="anchor" aria-hidden="true" href="#holdout-cross-validation">#</a></h2>
<p>The holdout cross-validation randomly splits the dataset into train and test data depending on data analysis.</p>
<p>In the case of holdout cross-validation, the dataset is randomly split into training and validation data. Generally, the split of training data is more than test data. The training data is used to induce the model and validation data is evaluates the performance of the model.
The more data is used to train the model, the better the model is. For the holdout cross-validation method, a good amount of data is isolated from training.</p>
<p><strong>Pros:</strong></p>
<ol>
<li>Same as previous.</li>
</ol>
<p><strong>Cons:</strong></p>
<ol>
<li>Not suitable for an imbalanced dataset.</li>
<li>A lot of data is isolated from training the model.</li>
</ol>
<h2 id="k-fold-cross-validation">k-fold cross-validation<a hidden class="anchor" aria-hidden="true" href="#k-fold-cross-validation">#</a></h2>
<p>In k-fold cross-validation, the original dataset is equally partitioned into $\mathrm{k}$ subparts or folds. Out of the k-folds or groups, for each iteration, one group is selected as validation data, and the remaining $(\mathrm{k}-1)$ groups are selected as training data.</p>
<p>The process is repeated for $\mathrm{k}$ times until each group is treated as validation and remaining as training data.</p>
<p>The final accuracy of the model is computed by taking the mean accuracy of the k-models validation data.
$$
\mathbf{a c c}<em>{\mathrm{cv}}=\sum</em>{\mathbf{i}=1}^{\mathrm{k}} \frac{\mathbf{a c c}_{\mathrm{i}}}{\mathbf{k}}
$$
<strong>Pros:</strong></p>
<ol>
<li>The model has low bias</li>
<li>Low time complexity</li>
<li>The entire dataset is utilized for both training and validation.</li>
</ol>
<p><strong>Cons:</strong></p>
<ol>
<li>Not suitable for an imbalanced dataset.</li>
</ol>
<h2 id="repeated-random-subsampling-validation">Repeated random subsampling validation<a hidden class="anchor" aria-hidden="true" href="#repeated-random-subsampling-validation">#</a></h2>
<p>Repeated random subsampling validation also referred to as Monte Carlo crossvalidation splits the dataset randomly into training and validation. Unlikely k-fold cross-validation split of the dataset into not in groups or folds but splits in this case in random.</p>
<p>The number of iterations is not fixed and decided by analysis. The results are then averaged over the splits.
$$
\mathbf{a c c}<em>{\mathrm{cv}}=\sum</em>{\mathbf{i}=1}^{\mathrm{k}} \frac{\mathbf{a c c}_{\mathrm{i}}}{\mathbf{k}}
$$
<strong>Pros:</strong></p>
<ol>
<li>The proportion of train and validation splits is not dependent on the number of iterations or partitions.</li>
</ol>
<p><strong>Cons:</strong></p>
<ol>
<li>Some samples may not be selected for either training or validation.</li>
<li>Not suitable for an imbalanced dataset.</li>
</ol>
<h2 id="stratified-k-fold-cross-validation">Stratified k-fold cross-validation<a hidden class="anchor" aria-hidden="true" href="#stratified-k-fold-cross-validation">#</a></h2>
<p>For all the cross-validation techniques discussed above, they may not work well with an imbalanced dataset. Stratified kfold cross-validation <strong>solved the problem of an imbalanced dataset.</strong></p>
<p>In Stratified k-fold cross-validation, the dataset is partitioned into $\mathrm{k}$ groups or folds such that the validation data has an equal number of instances of target class label. This ensures that one particular class is not over present in the validation or train data especially when the dataset is imbalanced.</p>
<p>The final score is computed by taking the mean of scores of each fold.</p>
<p><strong>Pros:</strong></p>
<ol>
<li>Works well for an imbalanced dataset.</li>
</ol>
<p><strong>Cons:</strong></p>
<ol>
<li>Now suitable for time series dataset.</li>
</ol>
<h2 id="time-series-cross-validation">Time Series cross-validation<a hidden class="anchor" aria-hidden="true" href="#time-series-cross-validation">#</a></h2>
<p>The order of the data is very important for time-series related problem. For timerelated dataset random split or k-fold split of data into train and validation may not yield good results.</p>
<p>For the time-series dataset, the split of data into train and validation is according to the time also referred to as forward chaining method or rolling cross-validation. For a particular iteration, the next instance of train data can be treated as validation data.</p>
<h2 id="nested-cross-validation">Nested cross-validation<a hidden class="anchor" aria-hidden="true" href="#nested-cross-validation">#</a></h2>
<p>In the case of k-fold and stratified k-fold cross-validation, we get a poor estimate of the error in training and test data.</p>
<p>Hyperparameter tuning is done separately in the earlier methods. When cross-
validation is used simultaneously for tuning the hyperparameters and generalizing the error estimate, nested cross-validation is required.</p>
<p>Nested Cross Validation can be applicable in both $k$-fold and stratified $k$-fold variants.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Cross-validation is used to compare and evaluate the performance of ML models. In this article, we have covered 8 crossvalidation techniques along with their pros and cons. k-fold and stratified k-fold crossvalidations are the most used techniques. Time series cross-validation works best with time series related problems.</p>
<p><a href="Medium.com">Reference</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="/tags/cross-validation/">Cross Validation</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="/">Landisland</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>

    <br>
    <span>All Conetents Are Licensed Under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="nooopener noreferrer" target="_blank">CC BY-NC 4.0</a> Unless Otherwise Stated</span>
    <br>
    

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
